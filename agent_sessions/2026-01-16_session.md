# Session Notes - 2026-01-16

## Session Focus
Redesigning the Success Library index from card format to table format, and establishing the build/generation architecture.

---

## Key Decisions Made

### 1. Index Format Change
- **Previous:** Card-based layout (verbose, repeating metadata blocks)
- **New:** Table format with all metadata columns
- **Reason:** Cards don't scale well with 300+ campaigns across 30-40 products

### 2. Build/Generate Architecture (Option C)
Instead of hardcoding everything in HTML, we're implementing a build system:

```
Source Files (what you edit)          Generated (don't edit)
─────────────────────────────         ─────────────────────────
metadata/success_library_index.json
         +                      ──►   index.html
code/VVD/VVD_ACQ_001.md              (table format with embedded code)
         │
         ▼
      build.py
```

**Why this approach:**
- Single source of truth (edit JSON and code files, not HTML)
- Scales to hundreds of metrics
- No risk of mismatch between files and display
- Works offline (static HTML output)

### 3. Combined Code Files (SQL + PySpark in one file)
- **Previous:** Separate `.sql` files per metric
- **New:** Single `.md` file per metric containing both SQL and PySpark
- **Format:** Markdown with code blocks

**Why both SQL and PySpark (not a wrapper):**
- Data Warehouse (relational DB) uses SQL with specific table paths
- Hive (Data Lake) uses PySpark via YARN/Spark with different table paths
- Same metric, two genuinely different implementations
- Can't wrap one in the other because addresses/paths are different

### 4. Version Control Approach
- **Option B selected:** Simple commit history, no review gate
- Git handles versioning automatically (who, when, what, why)
- Manual version labels in files for major revisions only (v1.0, v2.0)
- No Pull Request workflow required (trust + rollback capability)

### 5. Future: Excel Intake Process
- Analysts shouldn't edit JSON directly (error-prone)
- Future workflow: Excel template → Script → JSON
- Not implemented this session, noted for later

---

## Architecture Decisions Summary

| Aspect | Decision | Rationale |
|--------|----------|-----------|
| Index format | Table (not cards) | Scales better, less verbose |
| Code storage | Single .md file per metric | SQL + PySpark together, easier maintenance |
| HTML generation | Build script (build.py) | Single source of truth, no duplication |
| Version control | Git (Option B, no review gate) | Simple, trust-based, full history |
| Metadata intake | Future Excel → JSON script | Prevent analyst errors in JSON |

---

## File Changes Required

| File | Action | Status |
|------|--------|--------|
| `code/VVD/*.sql` | Convert to `.md` with SQL + PySpark | Pending |
| `index.html` | Rebuild as table format | Pending |
| `build.py` | Create new | Pending |
| `metadata/success_library_index.json` | Update code_path to .md | Pending |
| `docs/DESIGN_DECISIONS.md` | Create new | Pending |
| `README.md` | Rewrite with proper content | Pending |

---

## Directory Structure (Target)

```
NBA Souccess Library/
├── README.md                              # Project overview
├── index.html                             # GENERATED by build.py
├── build.py                               # Generates index.html
├── metadata/
│   └── success_library_index.json
├── code/
│   ├── old/                               # Reference only
│   └── VVD/
│       ├── VVD_ACQ_001.md                 # SQL + PySpark combined
│       ├── VVD_ACT_001.md
│       ├── VVD_USG_001.md
│       └── VVD_PRV_001.md
├── support/
│   └── success_library_project_context.md
├── docs/
│   └── DESIGN_DECISIONS.md
└── agent_sessions/
    └── *.md                               # Session notes
```

---

## Environment Context (from discussion)

Two data environments require two code implementations:

| Environment | Access Method | Example Path |
|-------------|---------------|--------------|
| Data Warehouse (Relational DB) | Pure SQL | `DGNV01.TACTIC_EVNT_IP_AR_HIST` |
| Hive (Data Lake) | PySpark via YARN/Spark | `hive_db.tactic_evnt_hist` |

This is why we need dedicated SQL and PySpark code blocks, not wrappers.

---

## Completed This Session

1. Created `docs/DESIGN_DECISIONS.md`
2. Converted `.sql` files to `.md` format (SQL + PySpark combined)
3. Created `build.py` script
4. Generated new table-based `index.html`
5. Created intake workflow:
   - `intake/template/intake_template.xlsx` - template for analysts
   - `intake/pending/` - drop intake files here
   - `intake/processed/` - processed files moved here
   - `excel_to_json.py` - processes intake forms

## Remaining / Future

1. Update `README.md` with usage instructions
2. Fill in actual business definitions and SQL/PySpark logic
3. Test intake workflow end-to-end

---

## User Preferences (Carried Forward)

- Do not assume - ask questions if unclear
- No praising/glazing - be pragmatic and critical
- Document decisions so context isn't lost between sessions

---

*Session date: 2026-01-16*
